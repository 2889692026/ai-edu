Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 自适应学习率算法

## AdaGrad

Adagrad是一个基于梯度的优化算法，它的主要功能是：它对不同的参数调整学习率，具体而言，对低频出现的参数进行大的更新，对高频出现的参数进行小的更新。因此，他很适合于处理稀疏数据。

在这之前，我们对于所有的参数使用相同的学习率进行更新。但 Adagrad 则不然，对不同的训练迭代次数t，AdaGrad 对每个参数都有一个不同的学习率。

### 输入和参数

- $\eta$ - 全局学习率
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-7
- r = 0 初始值
  
### 算法

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$
> 
> 累计平方梯度：$r_t = r_{t-1} + g_t \odot g_t$
> 
> 计算梯度更新：$\Delta \theta = {\eta \over \delta + \sqrt{r_t}} \odot g_t$
> 
> 更新参数：$\theta_t=\theta_{t-1} - \Delta \theta$

从AdaGrad算法中可以看出，随着算法不断迭代，r会越来越大，整体的学习率会越来越小。所以，一般来说AdaGrad算法一开始是激励收敛，到了后面就慢慢变成惩罚收敛，速度越来越慢。r值的变化如下：

0. $r_0 = 0$
1. $r_1=g_1^2$
2. $r_2=g_1^2+g_2^2$
3. $r_3=g_1^2+g_2^2+g_3^2$

在SGD中，随着梯度的增大，我们的学习步长应该是增大的。但是在AdaGrad中，随着梯度g的增大，我们的r也在逐渐的增大，且在梯度更新时r在分母上，也就是整个学习率是减少的，这是为什么呢？

这是因为随着更新次数的增大，我们希望学习率越来越慢。因为我们认为在学习率的最初阶段，我们距离损失函数最优解还很远，随着更新次数的增加，越来越接近最优解，所以学习率也随之变慢。

但是当某个参数梯度较小时，累积和也会小，那么更新速度就大。

经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。AdaGrade在某些深度学习模型上效果不错，但不是全部。

|初始步长值|损失函数值变化|拟合结果|
|---|---|---|
|0.1|<img src=".\Images\9\adagrad_loss_80.png">|<img src=".\Images\9\adagrad_result_80.png">|
|0.2|<img src=".\Images\9\adagrad_loss_81.png">|<img src=".\Images\9\adagrad_result_81.png">|
|0.3|<img src=".\Images\9\adagrad_loss_82.png">|<img src=".\Images\9\adagrad_result_82.png">|



## AlaDelta

Adadelta 法 [6] 是 Adagrad 法的一个延伸，它旨在解决它学习率不断单调下降的问题。相比计算之前所有梯度值的平方和，Adadelta 法仅计算在一个大小为 的时间区间内梯度值的累积和。
但该方法并不会存储之前 个梯度的平方值，而是将梯度值累积值按如下的方式递归地定义：它被定义为关于过去梯度值的衰减均值（decade average），当前时间的梯度均值是基于过去梯度均值和当前梯度值平方的加权平均，其中是类似上述动量项的权值。


## RMSProp - Root Mean Square Prop 

均方根弹性反向传播。

RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。RMSprop法要解决AdaGrad的学习率缩减问题。

### 输入和参数

- $\eta$ - 全局学习率，建议设置为0.001
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-8
- $\beta$ - 衰减速率，建议缺省取值0.9
- $r$ - 累积变量矩阵，与$\theta$尺寸相同，初始化为0
  
### 算法

> 计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

> 累计平方梯度：$r = \beta \cdot r + (1-\beta)(g_t \odot g_t)$

> 计算梯度更新：$\Delta \theta = {\eta \over \sqrt{r + \delta}} \odot g_t$

> 更新参数：$\theta_{t}=\theta_{t-1} - \Delta \theta$

RMSprop也将学习率除以了一个指数衰减的衰减均值。为了进一步优化损失函数在更新中存在摆动幅度过大的问题，并且进一步加快函数的收敛速度，RMSProp算法对权重W和偏置b的梯度使用了微分平方加权平均数，这种做法有利于消除了摆动幅度大的方向，用来修正摆动幅度，使得各个维度的摆动幅度都较小。另一方面也使得网络函数收敛更快。

其中，r值的变化如下：

0. $r_0 = 0$
1. $r_1=0.1g_1^2$
2. $r_2=0.9r_1+0.1g_2^2=0.09g_1^2+0.1g_2^2$
3. $r_3=0.9r_2+0.1g_3^2=0.081g_1^2+0.09g_2^2+0.1g_3^2$
 
与AdaGrad相比，r3要小很多，那么计算出来的学习率也不会衰减的太厉害。注意，在计算梯度更新时，分母开始时是个小于1的数，而且非常小，所以如果全局学习率设置过大的话，比如0.1，将会造成开始的步子迈得太大，而且久久不能收缩步伐，损失值也降不下来。

|初始学习率|损失函数值|拟合效果|
|---|---|---|
|0.1|<img src=".\Images\9\rmsprop_loss_80.png">|<img src=".\Images\9\rmsprop_result_80.png">|
|0.01|<img src=".\Images\9\rmsprop_loss_81.png">|<img src=".\Images\9\rmsprop_result_81.png">|
|0.005|<img src=".\Images\9\rmsprop_loss_82.png">|<img src=".\Images\9\rmsprop_result_82.png">|
|0.001|<img src=".\Images\9\rmsprop_loss_83.png">|<img src=".\Images\9\rmsprop_result_83.png">|

从上面的试验可以看出，0.01是本示例最好的设置。

## Adam - Adaptive Moment Estimation

计算每个参数的自适应学习率，相当于RMSProp + Momentum的效果。

### 输入和参数

- t - 当前迭代次数
- $\eta$ - 全局学习率，建议缺省值为0.001
- $\delta$ - 用于数值稳定的小常数，建议缺省值为1e-8
- $\beta_1, \beta_2$ - 矩估计的指数衰减速率，$\in[0,1)$，建议缺省值分别为0.9和0.999

### 算法

>计算梯度：$g_t = \nabla_\theta J(\theta_{t-1})$

>计数器加一：$t=t+1$

>更新有偏一阶矩估计：$m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t$

>更新有偏二阶矩估计：$v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2)(g_t \odot g_t)$

>修正一阶矩的偏差：$\hat m_t = m_t / (1-\beta_1^t)$

>修正二阶矩的偏差：$\hat v_t = v_t / (1-\beta_2^t)$

>计算梯度更新：$\Delta \theta = \eta \cdot \hat m_t /(\delta + \sqrt{\hat v_t})$

>更新参数：$\theta_t=\theta_{t-1} - \Delta \theta$


|初始学习率|损失函数值|拟合效果|
|---|---|---|
|0.1|<img src=".\Images\9\adam_loss_80.png">|<img src=".\Images\9\adam_result_80.png">|
|0.01|<img src=".\Images\9\adam_loss_81.png">|<img src=".\Images\9\adam_result_81.png">|
|0.005|<img src=".\Images\9\adam_loss_82.png">|<img src=".\Images\9\adam_result_82.png">|
|0.001|<img src=".\Images\9\adam_loss_83.png">|<img src=".\Images\9\adam_result_83.png">|

从上面的比较可以看到，初始学习率设置为0.01时比较理想。

# 各种不同的算法的比较图

在狭长山谷中的表现：

<img src=".\Images\9\1.gif">

在鞍点上的表现：

<img src=".\Images\9\2.gif">