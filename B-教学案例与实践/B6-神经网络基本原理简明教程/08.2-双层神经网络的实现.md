Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 双层神经网络的实现

先来观察一下样本：

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|x|0.606|0.129|0.643|...|0.199|
|y|-0.113|-0.269|-0.217|...|-0.281|

首先观察一下样本数据的范围，x是在[0,1]，y是[-1,1]，这样我们就不用做数据归一化了。这条线看起来像一条处于攻击状态的眼镜蛇！

<img src=".\Images\8\Sample.png">

## 定义神经网络结构

我们定义一个两层的神经网络，输入层不算，一个隐藏层，含4个神经元，一个输出层。

<img src=".\Images\8\nn.png">

## 输入层

输入层就是一个标量x值。

## 权重矩阵W1/B1

$$
W1=
\begin{pmatrix}
w_{1,1} \\
w_{2,1} \\
w_{3,1} \\
w_{4,1} \\
\end{pmatrix}
$$

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-4的权重线B1-1...B1-4应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-4的矩阵/向量。

$$
B1=
\begin{pmatrix}
b_{1,1} \\
b_{2,1} \\
b_{3,1} \\
b_{4,1} \\
\end{pmatrix}
$$


## 中间（隐）层

我们用一个4个神经元的网络来模拟函数，每个神经元的输入$Z1 = W1 \cdot X + B1$，我们在这里使用sigmoid函数，所以输出是$A1 = Sigmoid(Z1)$。

$$
Z1 = \begin{pmatrix}
z_{1,1} \\ 
z_{2,1} \\ 
z_{3,1} \\
z_{4,1} \end{pmatrix},
A1 = \begin{pmatrix}
a_{1,1} \\ 
a_{2,1} \\ 
a_{3,1} \\
a_{4,1} \end{pmatrix}
$$


## 权重矩阵W2/B2

W2的尺寸是1x4，B2的尺寸是1x1。
$$
W2=
\begin{pmatrix}w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \end{pmatrix}
$$

$$
B2=
\begin{pmatrix}
b_{1,1}
\end{pmatrix}
$$

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元，$Z2=W2 \cdot A1+B2$。


## 前向计算图

<img src=".\Images\8\fc.png">

至此，我们得到了以下一串公式：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$

### 前向计算代码

```Python
    def ForwardCalculationBatch(self, batch_x, dict_weights):
        W1 = dict_weights["W1"]
        B1 = dict_weights["B1"]
        W2 = dict_weights["W2"]
        B2 = dict_weights["B2"]
        # layer 1
        Z1 = np.dot(W1, batch_x) + B1
        A1 = CSigmoid().forward(Z1)
        # layer 2
        Z2 = np.dot(W2, A1) + B2
        A2 = Z2
        # keep cache for backward
        dict_cache ={"Z2": Z2, "A1": A1, "A2": A2, "Output": A2}
        return dict_cache
```


## 运行结果

按照上述代码的“标准”设置，我们可以得到以下结果：

<img src=".\Images\8\xavier_result.png"> 

代码位置：ch08, Level2
