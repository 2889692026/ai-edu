Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 如何用双曲正切函数做二分类函数

木头：在二分类问题中，使用Sigmoid函数作为分类函数，配合二分类交叉熵损失函数：

$$a(z) = \frac{1}{1 + e^{-z}} \tag{1}$$

$$J(w,b)=-\sum_{i=1}^m [y_i \log a_i + (1-y_i) \log (1-a_i)] \tag{2}$$

Sigmoid的函数曲线：

<img src=".\Images\7\sigmoid.png">

还有一个长得非常像的函数，双曲正切函数，图像如下：

$$a(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1 \tag{3}$$

<img src=".\Images\6\tanh.png">

木头：我的问题是，我们能不能用双曲正切函数作为分类函数呢？

铁柱：你的猜想的依据是什么呢？

木头：可以用y=0当分界线，把正负类分开啊！

铁柱：好想法！不过要提醒你，改变分类函数，需要改变其它一系列的网络设置，你可以一步步试试。

木头：好！

## 改变前向计算函数

```Python
# 实现双曲正切函数
def Tanh(z):
    a = 2.0 / (1.0 + np.exp(-2*z)) - 1.0
    return a

# 前向计算
def ForwardCalculationBatch(W, B, batch_X):
    Z = np.dot(W, batch_X) + B
    A = Tanh(Z)
    return A
```

正向计算容易改，反向传播肿么办？需要自己推公式啦！

对公式2求导：
$$
{\partial{J} \over \partial{a_i}}= -{1 \over m} \sum^m_{i=1} ({y_i \over a_i} - {1-y_i \over 1-a_i})=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \tag{4}
$$

对公式3求导：
$$
{\partial{a_i} \over \partial{z_i}}=(1-a_i)(1+a_i) \tag{5}
$$

用链式法则结合公式4,5：

$$
{\partial{J} \over \partial{z_i}}={\partial{J} \over \partial{a_i}}{\partial{a_i} \over \partial{z_i}}
$$

$$
=-{1 \over m} \sum^m_{i=1}{y_i-a_i \over a_i(1-a_i)} \cdot (1+a_i)(1-a_i)
$$

$$
={1 \over m} \sum^m_{i=1}{(a_i-y_i)(1+a_i) \over a_i}\tag{6}
$$


## 改变反向传播函数
```Python
def BackPropagationBatch(batch_X, batch_Y, A):
    m = batch_X.shape[1]
    dZ = (A - batch_Y) * (1 + A) / A
    # dZ列相加，即一行内的所有元素相加
    dB = dZ.sum(axis=1, keepdims=True)/m
    dW = np.dot(dZ, batch_X.T)/m
    return dW, dB
```

貌似这样就改好啦！运行吧：

```
Try_TanhAsBinaryClassifier.py:84: RuntimeWarning: divide by zero encountered in true_divide
  dZ = (A - batch_Y)*(1+A)/A
Try_TanhAsBinaryClassifier.py:51: RuntimeWarning: divide by zero encountered in log
  p = (1-Y) * np.log(1-A) + Y * np.log(A)
Try_TanhAsBinaryClassifier.py:51: RuntimeWarning: invalid value encountered in multiply
  p = (1-Y) * np.log(1-A) + Y * np.log(A)
```

哦！一脸黑线！出错了！看第一个错误应该是除数为0，即A值为0。

能不能把A从dZ中去掉呢？Tanh函数的导数是固定形式，所以需要修改交叉熵函数，使得损失函数对A的导数中含有(1+A)(1-A)，这样就可以消掉了。根据这样的思路把交叉熵函数修改一下，我们用简写方式，方便推导：

$$J=-[Y \log A + (1-Y) \log (1-A)] \tag{2}$$

改成：

$$J=-[(1+Y) \log (1+A) + (1-Y) \log (1-A)] \tag{7}$$

对公式7求导：

$$
{\partial J \over \partial A} = -({1+Y \over 1+A} - {1-Y \over 1-A})
$$

$$
= -{(1+Y)(1-A)-(1-Y)(1+A) \over (1+A)(1-A)}
$$

$$
= {2(A-Y) \over (1+A)(1-A)}
$$

结合公式5：

$$
{\partial J \over \partial Z}={2(A-Y) \over (1+A)(1-A)} (1+A)(1-A)=2(A-Y) \tag{8}
$$

好，现在我们需要同时修改损失函数和反向传播函数：

```Python
def CheckLoss(W, B, X, Y):
    m = X.shape[1]
    A = ForwardCalculationBatch(W,B,X)
    p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
    LOSS = np.sum(-p)  #binary classification
    loss = LOSS / m
    return loss

def BackPropagationBatch(batch_X, batch_Y, A):
    m = batch_X.shape[1]
    dZ = 2*(A - batch_Y)
    # dZ列相加，即一行内的所有元素相加
    dB = dZ.sum(axis=1, keepdims=True)/m
    dW = np.dot(dZ, batch_X.T)/m
    return dW, dB
```

运行！...... 二脸黑线！看打印信息和损失函数图，居然损失函数是个负数！

```
0 0 -0.2508651979213738 [[4.33771640e-05 6.77466989e-02]] [[0.2]]
0 10 -0.3751692737038894 [[-0.22229146  0.22029328]] [[0.32524338]]
0 20 -0.5131641136221597 [[-0.23654813  0.40283825]] [[0.80019324]]
```
<img src=".\Images\7\try_tanh_1.png">

木头：咋回事呢？难道是因为改了损失函数形式吗？

铁柱：当然啦！损失函数以前的形式是$-(Ylog(A)+(1-Y)log(1-A))$，由于使用Sigmoid函数，所以A的值永远在(0,1)之间，而Y的值是0或1，所以$log(A)$和$log(1-A)$都是负数，最终的结果是个正数。

改成1+A后，并且用tanh函数，其输出值A为(-1,1)，这样$log(1+A)$很有可能大于0，导致整个公式为负数。

木头：